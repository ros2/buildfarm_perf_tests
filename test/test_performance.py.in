# generated from buildfarm_perf_tests/test/test_performance.py.in
# generated code does not contain a copyright notice

from glob import glob
import os
import sys
import tempfile
import unittest

import launch
from launch import LaunchDescription
from launch_ros.actions import Node
import launch_testing

import matplotlib  # noqa: F401
import matplotlib.pyplot as plt
import numpy as np  # noqa: F401
import pandas as pd

plt.switch_backend('agg')

BYTE_TO_MBYTE = 1 / (1024*1024)
BYTE_TO_KBYTE = 1 / (1024)


def _cleanUpLogs(performance_log_prefix):
    for log in glob(performance_log_prefix + '*'):
        os.remove(log)


def _raw_to_csv(csv_data, csv_path):
    """
    Convert from the raw csv format to the csv for the Jenkins plot plugin.

    Do not change the order of the columns. The plot plugin indexes into the
    csv using the column number instead of the column name, because we're
    using the columns to identify which test produced the data.

    Changing the column names here will change the name of the line that
    appears on the plot.
    """
    dataframe = pd.read_csv(csv_data, skiprows=22, sep='[ \t]*,[ \t]*', engine='python')
    dataframe_agg = dataframe.agg(['max', 'mean', 'sum', 'median'])

    percentils_latency = dataframe['latency_mean (ms)'].describe(percentiles=[0.95])
    percentils_cpu_usage = dataframe['cpu_usage (%)'].describe(percentiles=[0.95])
    percentils_data_received = dataframe['data_received'].describe(percentiles=[0.95])
    values = [
        str(dataframe_agg.loc['mean', 'latency_mean (ms)']),
        str(dataframe_agg.loc['median', 'latency_mean (ms)']),
        str(percentils_latency.iloc[5]),
        str(dataframe_agg.loc['max', 'ru_maxrss']),
        str(dataframe_agg.loc['mean', 'received']),
        str(dataframe_agg.loc['mean', 'sent']),
        str(dataframe_agg.loc['sum', 'lost']),
        str(dataframe_agg.loc['mean', 'cpu_usage (%)']),
        str(percentils_cpu_usage.iloc[5]),
        str(dataframe_agg.loc['median', 'cpu_usage (%)']),
        str(dataframe_agg.loc['mean', 'data_received'] *BYTE_TO_MBYTE),
        str(dataframe_agg.loc['median', 'data_received'] * BYTE_TO_MBYTE),
        str(percentils_data_received.iloc[5] * BYTE_TO_MBYTE)
        ]

    with open(csv_path, 'w') as csv:
         csv.write(','.join(['@TEST_NAME@_@PERF_TEST_TOPIC@'] * len(values)) + '\n')
         csv.write(','.join(values) + '\n')

def _raw_to_png(csv_data, png_path):
    dataframe = pd.read_csv(csv_data, skiprows=22, sep='[ \t]*,[ \t]*', engine='python')
    pd.options.display.float_format = '{:.4f}'.format
    dataframe['maxrss (Mb)'] = dataframe['ru_maxrss'] * BYTE_TO_KBYTE
    dataframe.drop(list(dataframe.filter(regex='ru_')), axis=1, inplace=True)
    dataframe['latency_variance (ms) * 100'] = 100.0 * dataframe['latency_variance (ms)']
    percentils_latency = dataframe['latency_mean (ms)'].describe(percentiles=[0.95])
    dataframe['latency_p95 (ms)'] = percentils_latency.iloc[5]
    ax = dataframe[['T_experiment',
               'latency_min (ms)',
               'latency_max (ms)',
               'latency_mean (ms)',
               'latency_p95 (ms)',
               'latency_variance (ms) * 100',
               'maxrss (Mb)']].plot(x='T_experiment', secondary_y=['maxrss (Mb)'])

    plot_name = "One Process"

    if '@NUMBER_PROCESS@' == 2:
      plot_name = "Two Processes"

    plt.title('Performance ' + plot_name + ' tests latency\n@TEST_NAME@ @PERF_TEST_TOPIC@')
    ax.set_ylabel("ms")
    plt.savefig(png_path)

    dataframe['data_received Mbits/s'] = dataframe['data_received'] * BYTE_TO_MBYTE
    percentils_data_received = dataframe['data_received Mbits/s'].describe(percentiles=[0.95])
    dataframe['data_received Mbits/s (P95)'] = percentils_data_received.iloc[5]
    ax = dataframe[['T_experiment',  'data_received Mbits/s', 'data_received Mbits/s (P95)']].plot(x='T_experiment')
    plt.title('Performance ' + plot_name + ' Throughput (Mbits/s) Tests\n@TEST_NAME@ @PERF_TEST_TOPIC@')
    ax.set_ylabel("Mbits/s")
    plt.savefig(png_path[:-4] + "_throughput.png")

    ax = dataframe[['T_experiment', 'cpu_usage (%)']].plot(x='T_experiment')
    plt.title('Performance ' + plot_name + ' tests CPU usage (%)\n@TEST_NAME@ @PERF_TEST_TOPIC@')
    ax.set_ylabel("%")
    plt.savefig(png_path[:-4] + "_cpu_usage.png")

    ax = dataframe.plot(kind='bar', y=['received', 'sent', 'lost'])
    plt.title(plot_name + ' Received/Sent packets per second and Lost packets\n@TEST_NAME@ @PERF_TEST_TOPIC@')
    ax.set_ylabel("Number of packets")
    plt.savefig(png_path[:-4] + "_histogram.png")


def generate_test_description(ready_fn):
    performance_log_prefix = tempfile.mkstemp(
        prefix='performance_test_@TEST_NAME@_', text=True)[1]

    launch_description = []
    args = [
        '-c', '@COMM@',
        '-t', '@PERF_TEST_TOPIC@',
        '--max_runtime', '@PERF_TEST_MAX_RUNTIME@',
        "--ignore", "3",
        '-l', performance_log_prefix,
    ]

    sync_env = {}
    if '@SYNC_MODE@' == 'sync' and '@COMM@' != 'ROS2':
      args.append('--disable_async')

    nodes = []

    if @NUMBER_PROCESS@ == 2:
        args.append('--roundtrip_mode')
        args.append('Main')

        node_relay = Node(
          package='performance_test', node_executable='perf_test', output='log',
          arguments=[
              '-c', '@COMM@',
              '-t', '@PERF_TEST_TOPIC@',
              '--max_runtime', '@PERF_TEST_MAX_RUNTIME@',
              "--roundtrip_mode", "Relay",
          ],
        )
        launch_description.append(node_relay)
        nodes.append(node_relay)

    node_under_test = Node(
        package='performance_test', node_executable='perf_test', output='log',
        arguments=args,
    )
    nodes.append(node_under_test)

    launch_description.append(node_under_test)
    launch_description.append(launch_testing.util.KeepAliveProc())
    launch_description.append(launch.actions.OpaqueFunction(function=lambda context: ready_fn()))

    return LaunchDescription(launch_description), locals()


class PerformanceTestTermination(unittest.TestCase):

    def test_termination_@TEST_NAME@(self, nodes, proc_info):
        for node in nodes:
            proc_info.assertWaitForShutdown(
                process=node,
                timeout=(@PERF_TEST_MAX_RUNTIME@ * 2))


@launch_testing.post_shutdown_test()
class PerformanceTestResults(unittest.TestCase):

    def test_results_@TEST_NAME@(self, performance_log_prefix, nodes, proc_info):
        self.addCleanup(_cleanUpLogs, performance_log_prefix)

        for node in nodes:
            launch_testing.asserts.assertExitCodes(
                proc_info,
                [launch_testing.asserts.EXIT_OK],
                node,
            )

        performance_logs = glob(performance_log_prefix + '_*')
        if performance_logs:
            performance_report_csv = os.environ.get('PERFORMANCE_REPORT_CSV')
            if performance_report_csv:
                _raw_to_csv(performance_logs[0], performance_report_csv)
            else:
                print('No CSV report written - set PERFORMANCE_REPORT_CSV to write a report',
                      file=sys.stderr)

            performance_report_png = os.environ.get('PERFORMANCE_REPORT_PNG')
            if performance_report_png:
                _raw_to_png(performance_logs[0], performance_report_png)
            else:
                print('No PNG report written - set PERFORMANCE_REPORT_PNG to write a report',
                      file=sys.stderr)
        else:
            print('No report written - no performance log was produced', file=sys.stderr)
